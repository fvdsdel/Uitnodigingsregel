{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67d009d-0eb0-4c5f-a26b-8abfe5a1ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19354dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import .py scripts from subdirectories \n",
    "# Import script with basic data cleaning: drop duplicate rows and change NA values of numerical columns to the column mean\n",
    "from module.dataset import *\n",
    "# Import script that contains 3 models that train on the train dataset. Tuning is done with GridsearchCV. The models are Random Forest (RF),\n",
    "# Lasso Regression (lasso) and Support Vector Machines (SVM)\n",
    "from module.modeling import randomforestregressormodel_train, lassoregressionmodel_train, supportvectormachinemodel_train\n",
    "from module.modeling import randomforestregressormodel_pred, lassoregressionmodel_pred, supportvectormachinemodel_pred\n",
    "# Import python script that contains feature engineering. The first function checks a dataset for categorical columns and changes them\n",
    "# using dummy variables. The second function standardizes the data using a minmax scaler. This is needed for the lasso and SVM models\n",
    "from module.features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1905d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which settings to load\n",
    "settings_type = 'default'  # Change to 'custom' to load custom settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfea51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrain_models': False, 'separator': '\\t', 'dropout_column': 'Dropout', 'studentnumber_column': 'Studentnummer', 'save_method': 'xlsx', 'PROJ_ROOT': '.', 'DATA_DIR': './data', 'RAW_DATA_DIR': './data/raw', 'INTERIM_DATA_DIR': './data/interim', 'PROCESSED_DATA_DIR': './data/processed', 'EXTERNAL_DATA_DIR': './data/external', 'MODELS_DIR': './models', 'synth_data_dir_train': './data/raw/synth_data_train.csv', 'synth_data_dir_pred': './data/raw/synth_data_pred.csv', 'user_data_dir_train': './data/raw/user_data/train.csv', 'user_data_dir_pred': './data/raw/user_data/pred.csv', 'random_seed': 42, 'rf_parameters': {'bootstrap': [True, False], 'max_depth': [2, 3, 4], 'max_features': [3, 4, 5], 'min_samples_leaf': [3, 4, 5], 'min_samples_split': [2, 3, 5], 'n_estimators': [100, 200, 300]}, 'alpha_range': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9], 'svm_parameters': {'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1], 'kernel': ['rbf']}}\n"
     ]
    }
   ],
   "source": [
    "# Load config.yaml file \n",
    "config_file = 'config.yaml'\n",
    "\n",
    "def load_settings(config_file, settings_type = settings_type):\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    if settings_type == 'default':\n",
    "        settings = config['default_settings']\n",
    "    elif settings_type == 'custom':\n",
    "        settings = config['custom_settings']\n",
    "    else:\n",
    "        raise ValueError(\"No settings found. Choose 'default' or 'custom'.\")\n",
    "    return settings\n",
    "settings = load_settings(config_file, settings_type)\n",
    "\n",
    "# Apply settings dynamically\n",
    "globals().update(settings)\n",
    "\n",
    "print (settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b41b96a-b0f3-4e5b-b910-9b18e9bfc4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-uploaded synthetic datasets found and loaded\n"
     ]
    }
   ],
   "source": [
    "# Check if train.csv and pred.csv exist in user_data folder, otherwise load synthetic datasests\n",
    "# When user data is loaded and an error occurs here, please check if the sep = '\\t' needs to be changed in the config.yaml file to another separator like ',' or '.'\n",
    "# This should be the same as the separator used in your .csv file\n",
    "if os.path.exists(user_data_dir_train) and os.path.exists(user_data_dir_pred):\n",
    "    train_path,pred_path = user_data_dir_train, user_data_dir_pred\n",
    "    print ('User datasets found')\n",
    "else:\n",
    "    train_path,pred_path = synth_data_dir_train, synth_data_dir_pred\n",
    "    print ('Pre-uploaded synthetic datasets found')\n",
    "    \n",
    "train_df = pd.read_csv(train_path, sep = separator, engine='python')\n",
    "pred_df = pd.read_csv(pred_path, sep = separator, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8ffb85-24b6-4c3f-946a-c2350e2b4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data cleaning: drop rows that are duplicate and change any NA values to the average value of the column it's in. \n",
    "train_basic_cl = basic_cleaning (train_df)\n",
    "pred_basic_cl = basic_cleaning (pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b60dc-c2b5-427d-9801-64d696ebf001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if there are columns in which all rows have the same value and delete these columns from the train and predict datasets \n",
    "train_cleaned, pred_cleaned = remove_single_value_columns(train_basic_cl, pred_basic_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4e5ce-f888-4cd8-8f49-c5022c03494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function that changes categorical data into numerical data so it can be used as input for the models \n",
    "train_processed, pred_processed = convert_categorical_to_dummies(train_cleaned, pred_cleaned, dropout_column, separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d1dbe-2ffa-48a0-9557-852e7b4768a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function standardize_min_max to standardize the train and pred datasets using a min max scaler and save them as .csv files in the folder data/interim \n",
    "# These datasets can be used for the lasso and svm models, because reggression is sensitive to scaling \n",
    "train_df_sdd, pred_df_sdd = standardize_dataset(train_processed, pred_processed, dropout_column, separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23413d87-8933-4102-a422-067e6d96717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrain_models is False in the config.yaml file, loading the pre-trained models\n"
     ]
    }
   ],
   "source": [
    "# Code checks if retrain_models = True or False in config.yaml file. When using your own datasets, change retrain_models in the config.yaml file to True, so the models are trained on your own data. \n",
    "# Warning: training the models can take a long time depending on the size and contents of your data. \n",
    "if retrain_models == True:\n",
    "    print ('Training models on the data...')\n",
    "    best_rf_model = randomforestregressormodel_train(train_processed, random_seed, dropout_column, rf_parameters)\n",
    "    best_lasso_model = lassoregressionmodel_train(train_df_sdd, random_seed, dropout_column, alpha_range)\n",
    "    best_svm_model = supportvectormachinemodel_train(train_df_sdd, random_seed, dropout_column, svm_parameters)\n",
    "else:\n",
    "    print('retrain_models is False in the config.yaml file, loading the pre-trained models')\n",
    "# Folds = number of train/test splits of the dataset, candidates = models with different parameters and fits = folds * candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e4238-74a0-4991-8a44-6f2126323cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import code that loads the trained models and that can predict on the dataset\n",
    "# from module.modeling.predict import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a2e71d9-630a-48d4-b5e7-28dd5c22512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaded models to predict on the datasets. The lasso and SVM models use the standardized dataset ot predict an, but take the student numnbers from the \n",
    "# regular predict dataset. \n",
    "ranked_students_rf = randomforestregressormodel_pred (pred_processed, dropout_column, studentnumber_column)\n",
    "ranked_students_lasso = lassoregressionmodel_pred(pred_df_sdd, pred_processed, dropout_column, studentnumber_column)\n",
    "ranked_students_svm = supportvectormachinemodel_pred(pred_df_sdd, pred_processed, dropout_column, studentnumber_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00e0878c-5341-4c5d-beb6-bd0880cd1142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file saved as .xlsx in the /models/predictions folder\n"
     ]
    }
   ],
   "source": [
    "# Save the output files as either .xlsx or as three .csv files \n",
    "if save_method == 'xlsx':\n",
    "    writer = pd.ExcelWriter('models/predictions/ranked_students.xlsx', engine='xlsxwriter')\n",
    "    ranked_students_rf.to_excel(writer, sheet_name='Random Forest', startrow=0, startcol=0, index=False)\n",
    "    ranked_students_lasso.to_excel(writer, sheet_name='Lasso', startrow=0, startcol=0, index=False)\n",
    "    ranked_students_svm.to_excel(writer, sheet_name='Support Vector Machine', startrow=0, startcol=0, index=False)\n",
    "    writer.close()\n",
    "    print ('Output file saved as .xlsx in the /models/predictions folder')\n",
    "elif save_method == 'csv':\n",
    "    ranked_students_rf.to_csv('models/predictions/csv_output/ranked_students_rf.csv', sep='\\t', index=False)\n",
    "    ranked_students_lasso.to_csv('models/predictions/csv_output/ranked_students_lasso.csv', sep='\\t', index=False)\n",
    "    ranked_students_svm.to_csv('models/predictions/csv_output/ranked_students_svm.csv', sep='\\t', index=False)\n",
    "    print ('Output files saved as .csv in the /models/predictions/csv_output folder')\n",
    "else:\n",
    "    print('Invalid save method. For save_method in the config.yaml file, please fill in \"xlsx\" or \"csv\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00097337-44f8-406f-9aee-6ef1e07e8030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da26bb-8d1b-44fc-997e-84070aa9ffbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

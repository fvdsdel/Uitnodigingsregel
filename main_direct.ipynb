{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Local modules\n",
    "from module.data_loader import OsirisData\n",
    "from module.dataset import *\n",
    "from module.modeling import randomforestregressormodel_train, lassoregressionmodel_train, supportvectormachinemodel_train\n",
    "from module.modeling import randomforestregressormodel_pred, lassoregressionmodel_pred, supportvectormachinemodel_pred\n",
    "from module.features import *\n",
    "from module.settings import load_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which settings to load\n",
    "settings_type = 'custom'  # [default,custom] Change to 'custom' to load custom settings\n",
    "dataloader = \"db\" # Change to 'file' for file input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ddb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config.yaml'\n",
    "settings = load_settings(config_file, settings_type)\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if train.csv and pred.csv exist in user_data folder, otherwise load synthetic datasests\n",
    "# When user data is loaded and an error occurs here, please check if the sep = '\\t' needs to be changed in the config.yaml file to another separator like ',' or '.'\n",
    "# This should be the same as the separator used in your .csv file\n",
    "\n",
    "if dataloader == 'file':\n",
    "    if os.path.exists(settings.user_data_dir_train) and os.path.exists(settings.user_data_dir_pred):\n",
    "        train_path,pred_path = user_data_dir_train, user_data_dir_pred\n",
    "        print ('User datasets found')\n",
    "    else:\n",
    "        train_path,pred_path = settings.synth_data_dir_train, settings.synth_data_dir_pred\n",
    "        print ('Pre-uploaded synthetic datasets found')\n",
    "        \n",
    "    # train_df = pd.read_csv(train_path, sep = separator, engine='python')\n",
    "    pred_df = pd.read_csv(pred_path, sep = separator, engine='python')\n",
    "elif dataloader == 'db':\n",
    "    team=\"BB\"\n",
    "    pred_cohort = 2024\n",
    "    train_cohort_min = 2023\n",
    "    db = OsirisData(env_path=\"./\")\n",
    "    # train_df = db.get_dataset(team=\"BB\",min_cohort=train_cohort_min,max_cohort=pred_cohort-1)\n",
    "    pred_df = db.get_dataset(team=team,min_cohort=pred_cohort,max_cohort=pred_cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if settings.retrain_models == True:\n",
    "    if dataloader == 'file':\n",
    "        train_df = pd.read_csv(train_path, sep = separator, engine='python')\n",
    "    elif dataloader == 'db':\n",
    "        # pred_cohort = 2024\n",
    "        # train_cohort_min = 2023\n",
    "        # db = OsirisData(env_path=\"./\")\n",
    "        train_df = db.get_dataset(team=team,min_cohort=train_cohort_min,max_cohort=pred_cohort-1)\n",
    "        # pred_df = db.get_dataset(team=\"BB\",min_cohort=pred_cohort,max_cohort=pred_cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data cleaning: drop rows that are duplicate and change any NA values to the average value of the column it's in. \n",
    "if settings.retrain_models == True:\n",
    "    train_basic_cl = basic_cleaning (train_df)\n",
    "pred_basic_cl = basic_cleaning (pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if there are columns in which all rows have the same value and delete these columns from the train and predict datasets \n",
    "if settings.retrain_models == True:\n",
    "    train_cleaned, pred_cleaned = remove_single_value_columns(train_basic_cl, pred_basic_cl)\n",
    "else: \n",
    "    train_cleaned, pred_cleaned = remove_single_value_columns(pred_basic_cl, pred_basic_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned[settings.studentnumber_column]=train_cleaned[settings.studentnumber_column].astype(\"int\")\n",
    "pred_cleaned[settings.studentnumber_column]=pred_cleaned[settings.studentnumber_column].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function that changes categorical data into numerical data so it can be used as input for the models \n",
    "train_processed, pred_processed = convert_categorical_to_dummies(train_cleaned, pred_cleaned, settings.dropout_column, settings.separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function standardize_min_max to standardize the train and pred datasets using a min max scaler and save them as .csv files in the folder data/interim \n",
    "# These datasets can be used for the lasso and svm models, because reggression is sensitive to scaling \n",
    "train_df_sdd, pred_df_sdd = standardize_dataset(train_processed, pred_processed, settings.dropout_column, settings.separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code checks if retrain_models = True or False in config.yaml file. \n",
    "# When using your own datasets, change retrain_models in the config.yaml file to True, so the models are trained on your own data. \n",
    "# Warning: training the models can take a long time depending on the size and contents of your data. \n",
    "if settings.retrain_models == True:\n",
    "    print ('Training models on the data...')\n",
    "    best_rf_model = randomforestregressormodel_train(\n",
    "        train_processed, settings.random_seed\n",
    "        , settings.dropout_column, settings.rf_parameters)\n",
    "    best_lasso_model = lassoregressionmodel_train(\n",
    "        train_df_sdd, settings.random_seed\n",
    "        , settings.dropout_column, settings.lasso_parameters)\n",
    "    best_svm_model = supportvectormachinemodel_train(\n",
    "        train_df_sdd, settings.random_seed\n",
    "        , settings.dropout_column, settings.svm_parameters)\n",
    "else:\n",
    "    print('retrain_models is False in the config.yaml file, loading the pre-trained models')\n",
    "# Folds = number of train/test splits of the dataset, candidates = models with different parameters and fits = folds * candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaded models to predict on the datasets. \n",
    "# The lasso and SVM models use the standardized dataset ot predict an, but take the student numnbers from the \n",
    "# regular predict dataset. \n",
    "ranked_students_rf = randomforestregressormodel_pred (pred_processed, settings.dropout_column, settings.studentnumber_column)\n",
    "ranked_students_lasso = lassoregressionmodel_pred(pred_df_sdd, pred_processed, settings.dropout_column, settings.studentnumber_column)\n",
    "ranked_students_svm = supportvectormachinemodel_pred(pred_df_sdd, pred_processed, settings.dropout_column, settings.studentnumber_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output files as either .xlsx or as three .csv files \n",
    "if settings.save_method == 'xlsx':\n",
    "    writer = pd.ExcelWriter('models/predictions/ranked_students.xlsx', engine='xlsxwriter')\n",
    "    ranked_students_rf.to_excel(writer, sheet_name='Random Forest', startrow=0, startcol=0, index=False)\n",
    "    ranked_students_lasso.to_excel(writer, sheet_name='Lasso', startrow=0, startcol=0, index=False)\n",
    "    ranked_students_svm.to_excel(writer, sheet_name='Support Vector Machine', startrow=0, startcol=0, index=False)\n",
    "    writer.close()\n",
    "    print ('Output file saved as .xlsx in the /models/predictions folder')\n",
    "elif settings.save_method == 'csv':\n",
    "    ranked_students_rf.to_csv('models/predictions/csv_output/ranked_students_rf.csv', sep='\\t', index=False)\n",
    "    ranked_students_lasso.to_csv('models/predictions/csv_output/ranked_students_lasso.csv', sep='\\t', index=False)\n",
    "    ranked_students_svm.to_csv('models/predictions/csv_output/ranked_students_svm.csv', sep='\\t', index=False)\n",
    "    print ('Output files saved as .csv in the /models/predictions/csv_output folder')\n",
    "else:\n",
    "    print('Invalid save method. For save_method in the config.yaml file, please fill in \"xlsx\" or \"csv\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a5d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

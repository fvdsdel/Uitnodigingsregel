{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Local modules\n",
    "from module.data_loader import OsirisData\n",
    "from module.dataset import *\n",
    "from module.modeling import randomforestregressormodel_train, lassoregressionmodel_train, supportvectormachinemodel_train\n",
    "from module.modeling import randomforestregressormodel_pred, lassoregressionmodel_pred, supportvectormachinemodel_pred\n",
    "from module.features import *\n",
    "from module.settings import load_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which settings to load\n",
    "settings_type = 'custom'  # [default,custom] Change to 'custom' to load custom settings\n",
    "dataloader = \"db\" # Change to 'file' for file input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ddb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config.yaml'\n",
    "settings = load_settings(config_file, settings_type)\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cac45",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m train_cohort_min = \u001b[32m2023\u001b[39m\n\u001b[32m     18\u001b[39m db = OsirisData(env_path=\u001b[33m\"\u001b[39m\u001b[33m./\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m train_df = \u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteam\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmin_cohort\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_cohort_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_cohort\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_cohort\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m pred_df = db.get_dataset(team=\u001b[33m\"\u001b[39m\u001b[33mBB\u001b[39m\u001b[33m\"\u001b[39m,min_cohort=pred_cohort,max_cohort=pred_cohort)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fritssteege\\Documents\\Python\\DGO-WG3\\Uitnodigingsregel\\module\\data_loader\\osiris.py:126\u001b[39m, in \u001b[36mOsirisData.get_dataset\u001b[39m\u001b[34m(self, output_cols, team, min_cohort, max_cohort)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m,output_cols:\u001b[38;5;28mlist\u001b[39m = OUPUT_COLS,team=\u001b[38;5;28;01mNone\u001b[39;00m,min_cohort=\u001b[32m2022\u001b[39m,max_cohort=\u001b[32m2024\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_data_from_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteam\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteam\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_cohort\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_cohort\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_cohort\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_cohort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     cols = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_cols \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df.columns]\n\u001b[32m    128\u001b[39m     df = df[cols]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fritssteege\\Documents\\Python\\DGO-WG3\\Uitnodigingsregel\\module\\data_loader\\osiris.py:111\u001b[39m, in \u001b[36mOsirisData.load_data_from_db\u001b[39m\u001b[34m(self, team, min_cohort, max_cohort)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mself\u001b[39m.df_insch = \u001b[38;5;28mself\u001b[39m.get_inschrijvingen_data(team=team,min_cohort=min_cohort,max_cohort=max_cohort)\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minschrijvingen geladen...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28mself\u001b[39m.df_verz = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_verzuim_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28mself\u001b[39m.df_verz = \u001b[38;5;28mself\u001b[39m.df_verz[VZCOLS]\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mverzuimdata geladen...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fritssteege\\Documents\\Python\\DGO-WG3\\Uitnodigingsregel\\module\\data_loader\\osiris.py:87\u001b[39m, in \u001b[36mOsirisData.get_verzuim_data\u001b[39m\u001b[34m(self, path, p_where)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_verzuim_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, path=file_verzuim,p_where=\u001b[38;5;28;01mNone\u001b[39;00m) -> pd.DataFrame:\n\u001b[32m     86\u001b[39m     qpath = \u001b[38;5;28mself\u001b[39m._cwd/path\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdb_conn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_query_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_where\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fritssteege\\Documents\\Python\\DGO-WG3\\Uitnodigingsregel\\module\\data_loader\\oracle\\oracle_db.py:47\u001b[39m, in \u001b[36mOraDB.run_query_from_file\u001b[39m\u001b[34m(self, file, index_col, where)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where:\n\u001b[32m     46\u001b[39m     q = q + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m AND \u001b[39m\u001b[33m\"\u001b[39m + where\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fritssteege\\Documents\\Python\\DGO-WG3\\Uitnodigingsregel\\module\\data_loader\\oracle\\oracle_db.py:35\u001b[39m, in \u001b[36mOraDB.run_query\u001b[39m\u001b[34m(self, q, index_col)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Runs a query and returns a pandas dataframe\"\"\"\u001b[39;00m\n\u001b[32m     34\u001b[39m conn = \u001b[38;5;28mself\u001b[39m.get_connection()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m conn.close()\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index_col:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\sql.py:708\u001b[39m, in \u001b[36mread_sql\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    720\u001b[39m         _is_table_name = pandas_sql.has_table(sql)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\sql.py:2728\u001b[39m, in \u001b[36mSQLiteDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   2717\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_query\u001b[39m(\n\u001b[32m   2718\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2719\u001b[39m     sql,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2726\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2727\u001b[39m ) -> DataFrame | Iterator[DataFrame]:\n\u001b[32m-> \u001b[39m\u001b[32m2728\u001b[39m     cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2729\u001b[39m     columns = [col_desc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor.description]\n\u001b[32m   2731\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\sql.py:2664\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2662\u001b[39m cur = \u001b[38;5;28mself\u001b[39m.con.cursor()\n\u001b[32m   2663\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2664\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n\u001b[32m   2666\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\oracledb\\cursor.py:714\u001b[39m, in \u001b[36mCursor.execute\u001b[39m\u001b[34m(self, statement, parameters, suspend_on_success, **keyword_parameters)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28mself\u001b[39m._impl.suspend_on_success = suspend_on_success\n\u001b[32m    713\u001b[39m impl = \u001b[38;5;28mself\u001b[39m._impl\n\u001b[32m--> \u001b[39m\u001b[32m714\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m impl.fetch_vars \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    716\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Check if train.csv and pred.csv exist in user_data folder, otherwise load synthetic datasests\n",
    "# When user data is loaded and an error occurs here, please check if the sep = '\\t' needs to be changed in the config.yaml file to another separator like ',' or '.'\n",
    "# This should be the same as the separator used in your .csv file\n",
    "\n",
    "if dataloader == 'file':\n",
    "    if os.path.exists(settings.user_data_dir_train) and os.path.exists(settings.user_data_dir_pred):\n",
    "        train_path,pred_path = user_data_dir_train, user_data_dir_pred\n",
    "        print ('User datasets found')\n",
    "    else:\n",
    "        train_path,pred_path = settings.synth_data_dir_train, settings.synth_data_dir_pred\n",
    "        print ('Pre-uploaded synthetic datasets found')\n",
    "        \n",
    "    # train_df = pd.read_csv(train_path, sep = separator, engine='python')\n",
    "    pred_df = pd.read_csv(pred_path, sep = separator, engine='python')\n",
    "elif dataloader == 'db':\n",
    "    pred_cohort = 2024\n",
    "    train_cohort_min = 2023\n",
    "    db = OsirisData(env_path=\"./\")\n",
    "    # train_df = db.get_dataset(team=\"BB\",min_cohort=train_cohort_min,max_cohort=pred_cohort-1)\n",
    "    pred_df = db.get_dataset(team=\"BB\",min_cohort=pred_cohort,max_cohort=pred_cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if settings.retrain_models == True:\n",
    "    if dataloader == 'file':\n",
    "        train_df = pd.read_csv(train_path, sep = separator, engine='python')\n",
    "    elif dataloader == 'db':\n",
    "        pred_cohort = 2024\n",
    "        train_cohort_min = 2023\n",
    "        db = OsirisData(env_path=\"./\")\n",
    "        train_df = db.get_dataset(team=\"BB\",min_cohort=train_cohort_min,max_cohort=pred_cohort-1)\n",
    "        # pred_df = db.get_dataset(team=\"BB\",min_cohort=pred_cohort,max_cohort=pred_cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data cleaning: drop rows that are duplicate and change any NA values to the average value of the column it's in. \n",
    "if settings.retrain_models == True:\n",
    "    train_basic_cl = basic_cleaning (train_df)\n",
    "pred_basic_cl = basic_cleaning (pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if there are columns in which all rows have the same value and delete these columns from the train and predict datasets \n",
    "if settings.retrain_models == True:\n",
    "    train_cleaned, pred_cleaned = remove_single_value_columns(train_basic_cl, pred_basic_cl)\n",
    "else: \n",
    "    train_cleaned, pred_cleaned = remove_single_value_columns(pred_basic_cl, pred_basic_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned[settings.studentnumber_column]=train_cleaned[settings.studentnumber_column].astype(\"int\")\n",
    "pred_cleaned[settings.studentnumber_column]=pred_cleaned[settings.studentnumber_column].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function that changes categorical data into numerical data so it can be used as input for the models \n",
    "train_processed, pred_processed = convert_categorical_to_dummies(train_cleaned, pred_cleaned, settings.dropout_column, settings.separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function standardize_min_max to standardize the train and pred datasets using a min max scaler and save them as .csv files in the folder data/interim \n",
    "# These datasets can be used for the lasso and svm models, because reggression is sensitive to scaling \n",
    "train_df_sdd, pred_df_sdd = standardize_dataset(train_processed, pred_processed, settings.dropout_column, settings.separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code checks if retrain_models = True or False in config.yaml file. \n",
    "# When using your own datasets, change retrain_models in the config.yaml file to True, so the models are trained on your own data. \n",
    "# Warning: training the models can take a long time depending on the size and contents of your data. \n",
    "if settings.retrain_models == True:\n",
    "    print ('Training models on the data...')\n",
    "    best_rf_model = randomforestregressormodel_train(\n",
    "        train_processed, settings.random_seed\n",
    "        , settings.dropout_column, settings.rf_parameters)\n",
    "    best_lasso_model = lassoregressionmodel_train(\n",
    "        train_df_sdd, settings.random_seed\n",
    "        , settings.dropout_column, settings.lasso_parameters)\n",
    "    best_svm_model = supportvectormachinemodel_train(\n",
    "        train_df_sdd, settings.random_seed\n",
    "        , settings.dropout_column, settings.svm_parameters)\n",
    "else:\n",
    "    print('retrain_models is False in the config.yaml file, loading the pre-trained models')\n",
    "# Folds = number of train/test splits of the dataset, candidates = models with different parameters and fits = folds * candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaded models to predict on the datasets. \n",
    "# The lasso and SVM models use the standardized dataset ot predict an, but take the student numnbers from the \n",
    "# regular predict dataset. \n",
    "ranked_students_rf = randomforestregressormodel_pred (pred_processed, dropout_column, studentnumber_column)\n",
    "ranked_students_lasso = lassoregressionmodel_pred(pred_df_sdd, pred_processed, settings.dropout_column, settings.studentnumber_column)\n",
    "ranked_students_svm = supportvectormachinemodel_pred(pred_df_sdd, pred_processed, settings.dropout_column, settings.studentnumber_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output files as either .xlsx or as three .csv files \n",
    "if settings.save_method == 'xlsx':\n",
    "    writer = pd.ExcelWriter('models/predictions/ranked_students.xlsx', engine='xlsxwriter')\n",
    "    ranked_students_rf.to_excel(writer, sheet_name='Random Forest', startrow=0, startcol=0, index=False)\n",
    "    ranked_students_lasso.to_excel(writer, sheet_name='Lasso', startrow=0, startcol=0, index=False)\n",
    "    ranked_students_svm.to_excel(writer, sheet_name='Support Vector Machine', startrow=0, startcol=0, index=False)\n",
    "    writer.close()\n",
    "    print ('Output file saved as .xlsx in the /models/predictions folder')\n",
    "elif settings.save_method == 'csv':\n",
    "    ranked_students_rf.to_csv('models/predictions/csv_output/ranked_students_rf.csv', sep='\\t', index=False)\n",
    "    ranked_students_lasso.to_csv('models/predictions/csv_output/ranked_students_lasso.csv', sep='\\t', index=False)\n",
    "    ranked_students_svm.to_csv('models/predictions/csv_output/ranked_students_svm.csv', sep='\\t', index=False)\n",
    "    print ('Output files saved as .csv in the /models/predictions/csv_output folder')\n",
    "else:\n",
    "    print('Invalid save method. For save_method in the config.yaml file, please fill in \"xlsx\" or \"csv\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a5d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
